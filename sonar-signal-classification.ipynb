{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281e353b",
   "metadata": {},
   "source": [
    "# Sonar signal classification using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13336c64",
   "metadata": {},
   "source": [
    "The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file \"sonar.rocks\" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04e2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c3f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sonar_dataset.csv\",header= None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1491d9",
   "metadata": {},
   "source": [
    "#### Our dataset doesn't have any column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd54c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95259217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9       10      11      12      13      14      15      16      17  \\\n",
       "0  0.2111  0.1609  0.1582  0.2238  0.0645  0.0660  0.2273  0.3100  0.2999   \n",
       "1  0.2872  0.4918  0.6552  0.6919  0.7797  0.7464  0.9444  1.0000  0.8874   \n",
       "2  0.6194  0.6333  0.7060  0.5544  0.5320  0.6479  0.6931  0.6759  0.7551   \n",
       "3  0.1264  0.0881  0.1992  0.0184  0.2261  0.1729  0.2131  0.0693  0.2281   \n",
       "4  0.4459  0.4152  0.3952  0.4256  0.4135  0.4528  0.5326  0.7306  0.6193   \n",
       "\n",
       "       18      19      20      21      22      23      24      25      26  \\\n",
       "0  0.5078  0.4797  0.5783  0.5071  0.4328  0.5550  0.6711  0.6415  0.7104   \n",
       "1  0.8024  0.7818  0.5212  0.4052  0.3957  0.3914  0.3250  0.3200  0.3271   \n",
       "2  0.8929  0.8619  0.7974  0.6737  0.4293  0.3648  0.5331  0.2413  0.5070   \n",
       "3  0.4060  0.3973  0.2741  0.3690  0.5556  0.4846  0.3140  0.5334  0.5256   \n",
       "4  0.2032  0.4636  0.4148  0.4292  0.5730  0.5399  0.3161  0.2285  0.6995   \n",
       "\n",
       "       27      28      29      30      31      32      33      34      35  \\\n",
       "0  0.8080  0.6791  0.3857  0.1307  0.2604  0.5121  0.7547  0.8537  0.8507   \n",
       "1  0.2767  0.4423  0.2028  0.3788  0.2947  0.1984  0.2341  0.1306  0.4182   \n",
       "2  0.8533  0.6036  0.8514  0.8512  0.5045  0.1862  0.2709  0.4232  0.3043   \n",
       "3  0.2520  0.2090  0.3559  0.6260  0.7340  0.6120  0.3497  0.3953  0.3012   \n",
       "4  1.0000  0.7262  0.4724  0.5103  0.5459  0.2881  0.0981  0.1951  0.4181   \n",
       "\n",
       "       36      37      38      39      40      41      42      43      44  \\\n",
       "0  0.6692  0.6097  0.4943  0.2744  0.0510  0.2834  0.2825  0.4256  0.2641   \n",
       "1  0.3835  0.1057  0.1840  0.1970  0.1674  0.0583  0.1401  0.1628  0.0621   \n",
       "2  0.6116  0.6756  0.5375  0.4719  0.4647  0.2587  0.2129  0.2222  0.2111   \n",
       "3  0.5408  0.8814  0.9857  0.9167  0.6121  0.5006  0.3210  0.3202  0.4295   \n",
       "4  0.4604  0.3217  0.2828  0.2430  0.1979  0.2444  0.1847  0.0841  0.0692   \n",
       "\n",
       "       45      46      47      48      49      50      51      52      53  \\\n",
       "0  0.1386  0.1051  0.1343  0.0383  0.0324  0.0232  0.0027  0.0065  0.0159   \n",
       "1  0.0203  0.0530  0.0742  0.0409  0.0061  0.0125  0.0084  0.0089  0.0048   \n",
       "2  0.0176  0.1348  0.0744  0.0130  0.0106  0.0033  0.0232  0.0166  0.0095   \n",
       "3  0.3654  0.2655  0.1576  0.0681  0.0294  0.0241  0.0121  0.0036  0.0150   \n",
       "4  0.0528  0.0357  0.0085  0.0230  0.0046  0.0156  0.0031  0.0054  0.0105   \n",
       "\n",
       "       54      55      56      57      58      59 60  \n",
       "0  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032  R  \n",
       "1  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044  R  \n",
       "2  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078  R  \n",
       "3  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117  R  \n",
       "4  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094  R  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0c7e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4fefc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f199c8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d93619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R', 'M'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e97b0",
   "metadata": {},
   "source": [
    "### Here 'R' indicate rock and 'M' indicates metal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8dd0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    111\n",
       "R     97\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[60].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2837f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(60, axis=1)\n",
    "Y = df[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fbbd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(Y, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84abb21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R\n",
       "79   1\n",
       "101  0\n",
       "6    1\n",
       "43   1\n",
       "150  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7eeda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8cccb",
   "metadata": {},
   "source": [
    "### Using ANN without dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f518df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\hi\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 156 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6856 - acc: 0.5385\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.6664 - acc: 0.5962\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.6511 - acc: 0.6603\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.6427 - acc: 0.6474\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.6309 - acc: 0.6987\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.5926 - acc: 0.6923\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.5617 - acc: 0.7564\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.5349 - acc: 0.7436\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.5079 - acc: 0.7756\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.4638 - acc: 0.8397\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.4753 - acc: 0.7756\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.4311 - acc: 0.8269\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.4063 - acc: 0.8205\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3898 - acc: 0.8333\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3597 - acc: 0.8397\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3593 - acc: 0.8590\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3380 - acc: 0.8782\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3350 - acc: 0.8974\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3096 - acc: 0.8974\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3519 - acc: 0.8333\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.3078 - acc: 0.8782\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2906 - acc: 0.9167\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2965 - acc: 0.8590\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2663 - acc: 0.9103\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2738 - acc: 0.9103\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2427 - acc: 0.9231\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2315 - acc: 0.9359\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2206 - acc: 0.9295\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2350 - acc: 0.9359\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2243 - acc: 0.9359\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2189 - acc: 0.9359\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1943 - acc: 0.9487\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1754 - acc: 0.9615\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1864 - acc: 0.9551\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1647 - acc: 0.9615\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1480 - acc: 0.9679\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1834 - acc: 0.9167\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.2061 - acc: 0.9423\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1374 - acc: 0.9679\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1647 - acc: 0.9487\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1265 - acc: 0.9615\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1161 - acc: 0.9551\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.1194 - acc: 0.9615\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1116 - acc: 0.9744\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0993 - acc: 0.9679\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1094 - acc: 0.9744\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.1150 - acc: 0.9744\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0947 - acc: 0.9808\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0805 - acc: 0.9872\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0780 - acc: 0.9872\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0739 - acc: 0.9872\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0644 - acc: 0.9936\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0609 - acc: 0.9872\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0570 - acc: 0.9872\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0530 - acc: 0.9872\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0495 - acc: 0.9936\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0514 - acc: 0.9872\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0470 - acc: 0.9872\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0490 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0510 - acc: 0.9872\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0394 - acc: 0.9936\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0356 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0376 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0328 - acc: 0.9936\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0249 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0269 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0172 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 1ms/sample - loss: 0.0036 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e72318da0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be93b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 558us/sample - loss: 0.9014 - acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9014018911581773, 0.74999994]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb0bc2",
   "metadata": {},
   "source": [
    "## we can see here our model gets 100% accuracy in train set but only 74% in test set this is because of over fitting\n",
    "### Training Accuracy >>> Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05059b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.94415361e-06 9.75675166e-01 9.83121157e-01 1.29966109e-04\n",
      " 1.00000000e+00 9.96559322e-01 4.11564410e-01 1.00000000e+00\n",
      " 1.10754496e-04 1.00000000e+00]\n",
      "[0. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).reshape(-1)\n",
    "print(y_pred[:10])\n",
    "\n",
    "# round the values to nearest integer ie 0 or 1\n",
    "y_pred = np.round(y_pred)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08cc419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79        27\n",
      "           1       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.75        52\n",
      "   macro avg       0.77      0.74      0.74        52\n",
      "weighted avg       0.77      0.75      0.74        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7e462",
   "metadata": {},
   "source": [
    "## ANN with dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767390f",
   "metadata": {},
   "source": [
    "#### What is dropout layer?\n",
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. ... Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ba2d5",
   "metadata": {},
   "source": [
    "#### Why do we need dropout layer?\n",
    "It is Simple Way to Prevent Neural Networks from Overfitting. Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g. more nodes, may be required when using dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaee017",
   "metadata": {},
   "source": [
    "![](dropoutlayer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "553fa529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156 samples\n",
      "Epoch 1/100\n",
      "156/156 [==============================] - 0s 3ms/sample - loss: 0.7098 - acc: 0.4872\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.7056 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.7011 - acc: 0.5449\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6802 - acc: 0.4872\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.7106 - acc: 0.5449\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.7049 - acc: 0.5128\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6844 - acc: 0.5513\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6711 - acc: 0.5192\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6929 - acc: 0.5128\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6794 - acc: 0.4744\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6868 - acc: 0.5128\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6771 - acc: 0.5962\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.7150 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6893 - acc: 0.5385\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6885 - acc: 0.5128\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6634 - acc: 0.5833\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6713 - acc: 0.5577\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6558 - acc: 0.6218\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6485 - acc: 0.5705\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6647 - acc: 0.5385\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6417 - acc: 0.5962\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6633 - acc: 0.5833\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6555 - acc: 0.6410\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6511 - acc: 0.6218\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6674 - acc: 0.5962\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6072 - acc: 0.6859\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6424 - acc: 0.6795\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6201 - acc: 0.6731\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6311 - acc: 0.6410\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6292 - acc: 0.6538\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6180 - acc: 0.6603\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6137 - acc: 0.6474\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.6197 - acc: 0.6859\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5878 - acc: 0.7244\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5826 - acc: 0.7179\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5808 - acc: 0.7179\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5648 - acc: 0.7372\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5662 - acc: 0.7244\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5470 - acc: 0.7564\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5557 - acc: 0.7500\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5213 - acc: 0.7436\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5552 - acc: 0.6987\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5231 - acc: 0.7885\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5028 - acc: 0.7564\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4951 - acc: 0.7756\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4916 - acc: 0.7949\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5076 - acc: 0.7949\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.5356 - acc: 0.7500\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4836 - acc: 0.7756\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4662 - acc: 0.7949\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4986 - acc: 0.7756\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4443 - acc: 0.8269\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4793 - acc: 0.7500\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4266 - acc: 0.8462\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4433 - acc: 0.8397\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4771 - acc: 0.8013\n",
      "Epoch 57/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4185 - acc: 0.7885\n",
      "Epoch 58/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4206 - acc: 0.8397\n",
      "Epoch 59/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4466 - acc: 0.8333\n",
      "Epoch 60/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4311 - acc: 0.8077\n",
      "Epoch 61/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4639 - acc: 0.8333\n",
      "Epoch 62/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4226 - acc: 0.8397\n",
      "Epoch 63/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4121 - acc: 0.8333\n",
      "Epoch 64/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4316 - acc: 0.8462\n",
      "Epoch 65/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3955 - acc: 0.8141\n",
      "Epoch 66/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3941 - acc: 0.8462\n",
      "Epoch 67/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4124 - acc: 0.8205\n",
      "Epoch 68/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4067 - acc: 0.8397\n",
      "Epoch 69/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3605 - acc: 0.8333\n",
      "Epoch 70/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4387 - acc: 0.8077\n",
      "Epoch 71/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3936 - acc: 0.8654\n",
      "Epoch 72/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3887 - acc: 0.8462\n",
      "Epoch 73/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4170 - acc: 0.8397\n",
      "Epoch 74/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3805 - acc: 0.8590\n",
      "Epoch 75/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3688 - acc: 0.8910\n",
      "Epoch 76/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3699 - acc: 0.8462\n",
      "Epoch 77/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3290 - acc: 0.8526\n",
      "Epoch 78/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.4039 - acc: 0.8462\n",
      "Epoch 79/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3634 - acc: 0.8974\n",
      "Epoch 80/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3041 - acc: 0.8782\n",
      "Epoch 81/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.2723 - acc: 0.9038\n",
      "Epoch 82/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.2983 - acc: 0.8782\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3206 - acc: 0.8974\n",
      "Epoch 84/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3826 - acc: 0.8654\n",
      "Epoch 85/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3853 - acc: 0.8782\n",
      "Epoch 86/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3321 - acc: 0.8718\n",
      "Epoch 87/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3716 - acc: 0.8462\n",
      "Epoch 88/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3710 - acc: 0.8205\n",
      "Epoch 89/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3588 - acc: 0.8718\n",
      "Epoch 90/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3591 - acc: 0.8846\n",
      "Epoch 91/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3302 - acc: 0.8718\n",
      "Epoch 92/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3393 - acc: 0.8654\n",
      "Epoch 93/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3160 - acc: 0.8782\n",
      "Epoch 94/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.2551 - acc: 0.9231\n",
      "Epoch 95/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3726 - acc: 0.8141\n",
      "Epoch 96/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3382 - acc: 0.8846\n",
      "Epoch 97/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3519 - acc: 0.8910\n",
      "Epoch 98/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3823 - acc: 0.8846\n",
      "Epoch 99/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3860 - acc: 0.8846\n",
      "Epoch 100/100\n",
      "156/156 [==============================] - 0s 2ms/sample - loss: 0.3018 - acc: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27f6555d6d8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(60, input_dim=60, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train, y_train, epochs=100, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae73cde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 808us/sample - loss: 0.4564 - acc: 0.8077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4563522911988772, 0.8076923]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca0d50",
   "metadata": {},
   "source": [
    "#### Training Accuracy is still good but Test Accuracy Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9845d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.80495827e-06 7.81627655e-01 9.28987205e-01 6.73177373e-03\n",
      " 9.80226398e-01 9.05708611e-01 2.46319801e-01 9.78909492e-01\n",
      " 1.28197465e-02 9.86878574e-01]\n",
      "[0. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(X_test).reshape(-1)\n",
    "print(y_pred[:10])\n",
    "\n",
    "# round the values to nearest integer ie 0 or 1\n",
    "y_pred = np.round(y_pred)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0b3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81        27\n",
      "           1       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.81        52\n",
      "   macro avg       0.81      0.81      0.81        52\n",
      "weighted avg       0.81      0.81      0.81        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90ee2b",
   "metadata": {},
   "source": [
    "## You can see that by using dropout layer test accuracy increased from 0.74 to 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
